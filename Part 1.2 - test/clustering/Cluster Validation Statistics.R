#Cluster Validation Statistics
#In all the examples presented here, we'll apply k-means, PAM and hierarchical clustering. Note that, the functions used in this article can be applied to evaluate the validity of any other clustering methods.
#http://www.sthda.com/english/articles/29-cluster-validation-essentials/97-cluster-validation-statistics-must-know-methods/
#Internal measures for cluster validation
#     That is, we want the average distance within cluster to be as small as possible; and the average distance between clusters to be as large as possible.
#      In this section, we'll describe the two commonly used indices for assessing the goodness of clustering: the silhouette width and the Dunn index. These internal measure can be used also to determine the optimal number of clusters in the data.
#If the data set contains compact and well-separated clusters, the diameter of the clusters is expected to be small and the distance between the clusters is expected to be large. Thus, Dunn index should be maximized
#External measures for clustering validation
#External clustering validation, can be used to select suitable clustering algorithm for a given data set.
install.packages(c("factoextra", "fpc", "NbClust"))
library(factoextra)
library(fpc)
library(NbClust)
# Excluding the column "Species" at position 5
df <- iris[, -5]
# Standardize
df <- scale(df)
#------------------------------
#Clustering analysis
#eclust(x, FUNcluster = "kmeans", hc_metric = "euclidean", ...)
#x: numeric vector, data matrix or data frame
#FUNcluster: a clustering function including "kmeans", "pam", "clara", "fanny", "hclust", "agnes" and "diana". Abbreviation is allowed.
#hc_metric: character string specifying the metric to be used for calculating dissimilarities between observations. Allowed values are those accepted by the function dist() [including "euclidean", "manhattan", "maximum", "canberra", "binary", "minkowski"] and correlation based distance measures ["pearson", "spearman" or "kendall"]. Used only when FUNcluster is a hierarchical clustering function such as one of "hclust", "agnes" or "diana".

# K-means clustering
km.res <- eclust(df, "kmeans", k = 3, nstart = 25, graph = FALSE)
# Visualize k-means clusters
fviz_cluster(km.res, geom = "point", ellipse.type = "norm",
             palette = "jco", ggtheme = theme_minimal())
# Hierarchical clustering
hc.res <- eclust(df, "hclust", k = 3, hc_metric = "euclidean", 
                 hc_method = "ward.D2", graph = FALSE)
# Visualize dendrograms
fviz_dend(hc.res, show_labels = FALSE,
          palette = "jco", as.ggplot = TRUE)
#-------------------------------------------------
#Cluster validation
#---------------
#Silhouette plot
#Recall that the silhouette coefficient (Si) measures how similar an object i is to the the other objects in its own cluster versus those in the neighbor cluster. Si values range from 1 to - 1:
#A value of Si close to 1 indicates that the object is well clustered. In the other words, the object i is similar to the other objects in its group.
#A value of Si close to -1 indicates that the object is poorly clustered, and that assignment to some other cluster would probably improve the overall results.

fviz_silhouette(km.res, palette = "jco",ggtheme = theme_classic())
# Silhouette information
silinfo <- km.res$silinfo
names(silinfo)
# Silhouette widths of each observation
head(silinfo$widths[, 1:3], 10)
# Average silhouette width of each cluster
silinfo$clus.avg.widths
# The total average (mean of all individual silhouette widths)
silinfo$avg.width
# The size of each clusters
km.res$siz
# Silhouette width of observation
sil <- km.res$silinfo$widths[, 1:3]
# Objects with negative silhouette
neg_sil_index <- which(sil[, 'sil_width'] < 0)
sil[neg_sil_index, , drop = FALSE]
#It can be seen that several samples, in cluster 2, have a negative silhouette coefficient. This means that they are not in the right cluster.
#We can find the name of these samples and determine the clusters they are closer (neighbor cluster),
#-------------------------
#Computing Dunn index and other cluster validation statistics
#The function cluster.stats() [fpc package] and the function NbClust() [in NbClust package] can be used to compute Dunn index and many other indices.
#cluster.stats(d = NULL, clustering, al.clustering = NULL)
#d: a distance object between cases as generated by the dist() function
#clustering: vector containing the cluster number of each observation
#alt.clustering: vector such as for clustering, indicating an alternative clustering

# we'll compute the clustering quality statistics for k-means. Look at the within.cluster.ss (within clusters sum of squares), the average.within (average distance within clusters) and clus.avg.silwidths (vector of cluster average silhouette widths)
library(fpc)
# Statistics for k-means clustering
km_stats <- cluster.stats(dist(df),  km.res$cluster)
# Dun index
km_stats$dunn
km_stats
#--------------------------------------------------------------
#External clustering validation
#there are two indexes to assess the similarity of two clustering, namely the corrected Rand index and Meila's VI.
#Let start by computing a cross-tabulation between k-means clusters and the reference Species labels:
table(iris$Species, km.res$cluster)
#All setosa species (n = 50) has been classified in cluster 1
#A large number of versicor species (n = 39 ) has been classified in cluster 3. Some of them ( n = 11) have been classified in cluster 2.
#A large number of virginica species (n = 36 ) has been classified in cluster 2. Some of them (n = 14) have been classified in cluster 3.

#It's possible to quantify the agreement between Species and k-means clusters using either the corrected Rand index and Meila's VI provided as follow
library("fpc")
# Compute cluster stats
species <- as.numeric(iris$Species)
clust_stats <- cluster.stats(d = dist(df), species, km.res$cluster)
# Corrected Rand index
clust_stats$corrected.rand
# VI
clust_stats$vi
#RESULT::The corrected Rand index provides a measure for assessing the similarity between two partitions, adjusted for chance. Its range is -1 (no agreement) to 1 (perfect agreement).
#        Agreement between the specie types and the cluster solution is 0.62 using Rand index and 0.748 using Meila's VI.
#--------------------------------------------------
#The same analysis can be computed for both PAM and hierarchical clustering:
# Agreement between species and pam clusters
pam.res <- eclust(df, "pam", k = 3, graph = FALSE)
table(iris$Species, pam.res$cluster)
cluster.stats(d = dist(iris.scaled),species, pam.res$cluster)$vi
# Agreement between species and HC clusters
res.hc <- eclust(df, "hclust", k = 3, graph = FALSE)
table(iris$Species, res.hc$cluster)
cluster.stats(d = dist(iris.scaled),species, res.hc$cluster)$vi
#External clustering validation, can be used to select suitable clustering algorithm for a given data set.
